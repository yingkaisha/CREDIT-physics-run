{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa08066-4289-4781-902c-7e83e244857d",
   "metadata": {},
   "source": [
    "# Interpolate ERA5 pressure level data to 1 deg\n",
    "\n",
    "This notebook performs conservative interpolation from 0.25 degree to 1.0 degree and prepares the dataset used in the paper.\n",
    "\n",
    "## Contents\n",
    "* **Main_file_groups**: upper-air, single-level, and diagnostic variables.\n",
    "* **Static & physics file**: static variables, required file for running physics-based schemes.\n",
    "* **Cloud and humidity file group**: ERA5 humidity components (not used in the paper).\n",
    "* **Bilinear interpolation**: a reference to compare againt the conservative interpolation (not used in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2adef9b2-b66a-4d4f-9c83-79fe47dce942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import dask\n",
    "import zarr\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa740e32-0024-4dec-b2c9-ce7c7f92f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d865a2c-583c-476e-9ea3-24986664bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8bd5e5-8eed-4e40-9606-cbf204127d6e",
   "metadata": {},
   "source": [
    "## Conservative interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b71b8b-bfef-4d40-b610-fa7e9d2ffee4",
   "metadata": {},
   "source": [
    "### Main file groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da6d518-0c0a-478e-a34f-96999ce81e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.realpath('../libs/'))\n",
    "import interp_utils as iu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29d2296b-c828-4cc6-b6b8-f2f361b6e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.realpath('data_config_6h.yml')\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)\n",
    "\n",
    "year = 1979 # test on one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a17588f-9ba6-4e07-9036-18dc36db628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = conf['zarr_opt']['save_loc']\n",
    "base_dir_1deg = conf['zarr_opt']['save_loc_1deg']\n",
    "\n",
    "# load all 0.25 deg ERA5 data\n",
    "zarr_name_surf = base_dir+'surf/ERA5_plevel_6h_surf_{}.zarr'\n",
    "zarr_name_surf_extra = base_dir+'surf/ERA5_plevel_6h_surf_extend_{}.zarr'\n",
    "zarr_name_accum = base_dir+'accum/ERA5_plevel_6h_accum_{}.zarr'\n",
    "zarr_name_forcing = base_dir+'forcing/ERA5_plevel_6h_forcing_{}.zarr'\n",
    "zarr_name_upper = base_dir+'upper_air/ERA5_plevel_6h_upper_air_{}.zarr'\n",
    "zarr_name_upper_Q = base_dir+'upper_air/ERA5_plevel_6h_Q_{}.zarr'\n",
    "zarr_name_static = base_dir+'static/ERA5_plevel_6h_static.zarr'\n",
    "\n",
    "ds_surf = xr.open_zarr(zarr_name_surf.format(year))\n",
    "ds_surf_extra = xr.open_zarr(zarr_name_surf_extra.format(year))\n",
    "ds_accum = xr.open_zarr(zarr_name_accum.format(year))\n",
    "ds_forcing = xr.open_zarr(zarr_name_forcing.format(year))\n",
    "ds_upper = xr.open_zarr(zarr_name_upper.format(year))\n",
    "ds_upper_Q = xr.open_zarr(zarr_name_upper_Q.format(year))\n",
    "ds_static = xr.open_zarr(zarr_name_static)\n",
    "\n",
    "# merge all and drop SST (this var is not needed)\n",
    "ds_merge = xr.merge([ds_surf, ds_accum, ds_forcing, ds_upper, ds_upper_Q, ds_surf_extra])\n",
    "ds_merge = ds_merge.drop_vars('SSTK')\n",
    "\n",
    "# ======================================================================================= #\n",
    "# 0.25 deg to 1 deg interpolation using conservative approach\n",
    "\n",
    "# Define the target 1-degree grid\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)\n",
    "target_grid = iu.Grid.from_degrees(lon_1deg, lat_1deg)\n",
    "\n",
    "lon_025deg = ds_merge['longitude'].values\n",
    "lat_025deg = ds_merge['latitude'].values[::-1]\n",
    "source_grid = iu.Grid.from_degrees(lon_025deg, lat_025deg)\n",
    "\n",
    "regridder = iu.ConservativeRegridder(source=source_grid, target=target_grid)\n",
    "\n",
    "ds_merge = ds_merge.chunk({'longitude': -1, 'latitude': -1})\n",
    "ds_merge_1deg = regridder.regrid_dataset(ds_merge)\n",
    "\n",
    "# Reorder the dimensions for all variables in ds_merge_1deg\n",
    "for var in ds_merge_1deg.data_vars:\n",
    "    # Get the current dimensions of the variable\n",
    "    current_dims = ds_merge_1deg[var].dims\n",
    "    \n",
    "    # If both 'latitude' and 'longitude' are present, reorder them\n",
    "    if 'latitude' in current_dims and 'longitude' in current_dims:\n",
    "        # New order: move 'latitude' and 'longitude' to the first two positions, preserve other dimensions\n",
    "        new_order = [dim for dim in current_dims if dim not in ['latitude', 'longitude']] + ['latitude', 'longitude']\n",
    "        \n",
    "        # Transpose the variable to the new order\n",
    "        ds_merge_1deg[var] = ds_merge_1deg[var].transpose(*new_order)\n",
    "\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)\n",
    "\n",
    "# Add latitude and longitude as coordinates to ds_merge_1deg\n",
    "ds_merge_1deg = ds_merge_1deg.assign_coords({\n",
    "    'latitude': lat_1deg,\n",
    "    'longitude': lon_1deg\n",
    "})\n",
    "\n",
    "# flip latitude from -90 --> 90 to 90 --> -90\n",
    "ds_merge_1deg = ds_merge_1deg.isel(latitude=slice(None, None, -1))\n",
    "\n",
    "# float64 --> float32\n",
    "ds_merge_1deg = ds_merge_1deg.astype(\n",
    "    {var: np.float32 for var in ds_merge_1deg if ds_merge_1deg[var].dtype == np.float64})\n",
    "\n",
    "# ======================================================================================= #\n",
    "# process land-sea mask and sea ice\n",
    "land_sea_mask = ds_static['land_sea_mask']\n",
    "\n",
    "# Define the target 1-degree grid\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)[::-1]\n",
    "\n",
    "# Create target grid as an xarray Dataset\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        'latitude': (['latitude'], lat_1deg),\n",
    "        'longitude': (['longitude'], lon_1deg)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the regridder object for bilinear interpolation\n",
    "regridder = xe.Regridder(ds_merge, ds_out, 'nearest_s2d')\n",
    "\n",
    "# Apply the regridding to interpolate all variables\n",
    "land_sea_mask_1deg = regridder(land_sea_mask)\n",
    "\n",
    "# combine CI and land-sea mask\n",
    "sea_ice_cover = ds_merge_1deg['CI']\n",
    "land_sea_mask_expanded = land_sea_mask_1deg.broadcast_like(sea_ice_cover)\n",
    "\n",
    "land_sea_CI_mask = xr.where(\n",
    "    (land_sea_mask_expanded == 0) & (sea_ice_cover > 0),\n",
    "    -sea_ice_cover,\n",
    "    land_sea_mask_expanded\n",
    ")\n",
    "\n",
    "ds_merge_1deg['land_sea_CI_mask'] = land_sea_CI_mask\n",
    "ds_merge_1deg = ds_merge_1deg.drop_vars('CI')\n",
    "\n",
    "# Convert latitude, longitude, and level coordinates to float32\n",
    "ds_merge_1deg = ds_merge_1deg.assign_coords({\n",
    "    'latitude': ds_merge_1deg['latitude'].astype(np.float32),\n",
    "    'longitude': ds_merge_1deg['longitude'].astype(np.float32),\n",
    "    'level': ds_merge_1deg['level'].astype(np.float32)\n",
    "})\n",
    "\n",
    "# ========================================================================== #\n",
    "# chunking\n",
    "varnames = list(ds_merge_1deg.keys())\n",
    "varname_4D = ['U', 'V', 'T', 'Z', 'Q', 'specific_total_water']\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    if var in varname_4D:\n",
    "        ds_merge_1deg[var] = ds_merge_1deg[var].chunk(conf['zarr_opt']['chunk_size_4d_1deg'])\n",
    "    else:\n",
    "        ds_merge_1deg[var] = ds_merge_1deg[var].chunk(conf['zarr_opt']['chunk_size_3d_1deg'])\n",
    "\n",
    "# zarr encodings\n",
    "dict_encoding = {}\n",
    "\n",
    "chunk_size_3d = dict(chunks=(conf['zarr_opt']['chunk_size_3d_1deg']['time'],\n",
    "                             conf['zarr_opt']['chunk_size_3d_1deg']['latitude'],\n",
    "                             conf['zarr_opt']['chunk_size_3d_1deg']['longitude']))\n",
    "\n",
    "chunk_size_4d = dict(chunks=(conf['zarr_opt']['chunk_size_4d_1deg']['time'],\n",
    "                             conf['zarr_opt']['chunk_size_4d_1deg']['level'],\n",
    "                             conf['zarr_opt']['chunk_size_4d_1deg']['latitude'],\n",
    "                             conf['zarr_opt']['chunk_size_4d_1deg']['longitude']))\n",
    "\n",
    "compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    if var in varname_4D:\n",
    "        dict_encoding[var] = {'compressor': compress, **chunk_size_4d}\n",
    "    else:\n",
    "        dict_encoding[var] = {'compressor': compress, **chunk_size_3d}\n",
    "\n",
    "# ========================================================================== #\n",
    "# save\n",
    "\n",
    "save_name = base_dir_1deg + 'all_in_one/ERA5_plevel_1deg_6h_{}_conserve.zarr'.format(year)\n",
    "# ds_merge_1deg.to_zarr(save_name, mode='w', consolidated=True, compute=True, encoding=dict_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290598ab-b0cb-4ae4-b217-cf8c86f328ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/derecho/scratch/ksha/CREDIT_data/ERA5_plevel_1deg/all_in_one/ERA5_plevel_1deg_6h_1979_conserve.zarr'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35fe7de-f8bd-4651-82a2-c04f8763ef0b",
   "metadata": {},
   "source": [
    "### Static & physics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3daf3e6-a8e8-4049-a2d3-8e47e5d54940",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.realpath('data_config_6h.yml')\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "723c08e9-d27f-4b17-ad17-550a103faa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_1deg = conf['zarr_opt']['save_loc_1deg']\n",
    "base_dir = conf['ARCO']['save_loc'] + 'static/' \n",
    "static_name = base_dir + conf['ARCO']['prefix'] + '_static.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66a38e25-0d2c-4f95-88cd-3f891c286ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_static = xr.open_zarr(static_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "991dfd7b-5f90-47d4-889d-59b32ed47643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================== #\n",
    "# geopotential at surface\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)\n",
    "target_grid = iu.Grid.from_degrees(lon_1deg, lat_1deg)\n",
    "\n",
    "lon_025deg = ds_static['longitude'].values\n",
    "lat_025deg = ds_static['latitude'].values[::-1]\n",
    "source_grid = iu.Grid.from_degrees(lon_025deg, lat_025deg)\n",
    "\n",
    "regridder = iu.ConservativeRegridder(source=source_grid, target=target_grid)\n",
    "\n",
    "ds_static = ds_static.chunk({'longitude': -1, 'latitude': -1})\n",
    "ds_static_1deg = regridder.regrid_dataset(ds_static)\n",
    "\n",
    "for var in ds_static_1deg.data_vars:\n",
    "    # Get the current dimensions of the variable\n",
    "    current_dims = ds_static_1deg[var].dims\n",
    "    \n",
    "    # If both 'latitude' and 'longitude' are present, reorder them\n",
    "    if 'latitude' in current_dims and 'longitude' in current_dims:\n",
    "        # New order: move 'latitude' and 'longitude' to the first two positions, preserve other dimensions\n",
    "        new_order = [dim for dim in current_dims if dim not in ['latitude', 'longitude']] + ['latitude', 'longitude']\n",
    "        \n",
    "        # Transpose the variable to the new order\n",
    "        ds_static_1deg[var] = ds_static_1deg[var].transpose(*new_order)\n",
    "\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)\n",
    "\n",
    "# Add latitude and longitude as coordinates to ds_static_1deg\n",
    "ds_static_1deg = ds_static_1deg.assign_coords({\n",
    "    'latitude': lat_1deg,\n",
    "    'longitude': lon_1deg\n",
    "})\n",
    "\n",
    "# flip latitude from -90 --> 90 to 90 --> -90\n",
    "ds_static_1deg = ds_static_1deg.isel(latitude=slice(None, None, -1))\n",
    "\n",
    "# ================================================================================== #\n",
    "# normalized geopotential at surface\n",
    "\n",
    "# normalize 'geopotential_at_surface\n",
    "mean_val = float(ds_static_1deg['geopotential_at_surface'].mean(skipna=False))\n",
    "std_val = float(ds_static_1deg['geopotential_at_surface'].std(skipna=False))\n",
    "z_norm = (ds_static_1deg['geopotential_at_surface'] - mean_val)/std_val\n",
    "ds_static_1deg['z_norm'] = z_norm\n",
    "\n",
    "# ======================================================================================= #\n",
    "# process land-sea mask and sea ice\n",
    "land_sea_mask = ds_static['land_sea_mask']\n",
    "\n",
    "# Define the target 1-degree grid\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)[::-1]\n",
    "\n",
    "# Create target grid as an xarray Dataset\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        'latitude': (['latitude'], lat_1deg),\n",
    "        'longitude': (['longitude'], lon_1deg)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the regridder object for bilinear interpolation\n",
    "regridder = xe.Regridder(ds_static, ds_out, 'nearest_s2d')\n",
    "\n",
    "# Apply the regridding to interpolate all variables\n",
    "land_sea_mask_1deg = regridder(land_sea_mask)\n",
    "ds_static_1deg['land_sea_mask'] = land_sea_mask_1deg\n",
    "\n",
    "# ======================================================================================= #\n",
    "# process soil type\n",
    "soil_type = ds_static['soil_type']\n",
    "\n",
    "soil_type_1deg = regridder(soil_type)\n",
    "ds_static_1deg['soil_type'] = soil_type_1deg\n",
    "\n",
    "# ================================================================================== #\n",
    "# physics variables\n",
    "\n",
    "# lon1d and lat1d are 1D\n",
    "lon1d = ds_static_1deg['longitude'].values\n",
    "lat1d = ds_static_1deg['latitude'].values\n",
    "\n",
    "# Generate 2D latitude and longitude arrays\n",
    "lon2d, lat2d = np.meshgrid(lon1d, lat1d)\n",
    "\n",
    "# Define pressure levels (p_level) as a new dimension\n",
    "p_level = np.array([   100.,    200.,    300.,    500.,    700.,   1000.,   2000.,\n",
    "                      3000.,   5000.,   7000.,  10000.,  12500.,  15000.,  17500.,\n",
    "                     20000.,  22500.,  25000.,  30000.,  35000.,  40000.,  45000.,\n",
    "                     50000.,  55000.,  60000.,  65000.,  70000.,  75000.,  77500.,\n",
    "                     80000.,  82500.,  85000.,  87500.,  90000.,  92500.,  95000.,\n",
    "                     97500., 100000.])\n",
    "\n",
    "# Add 2D longitude and latitude arrays to the dataset\n",
    "ds_static_1deg['lon2d'] = xr.DataArray(lon2d, dims=('latitude', 'longitude'))\n",
    "ds_static_1deg['lat2d'] = xr.DataArray(lat2d, dims=('latitude', 'longitude'))\n",
    "\n",
    "# get plevel coordiante from upper air\n",
    "ds_example = xr.open_zarr(base_dir_1deg+'all_in_one/ERA5_plevel_1deg_6h_1979_conserve.zarr')\n",
    "\n",
    "# Add pressure levels as a new variable with its own dimension\n",
    "ds_static_1deg['p_level'] = xr.DataArray(p_level, dims=('level'))\n",
    "ds_static_1deg = ds_static_1deg.assign_coords(level=('level', p_level))\n",
    "\n",
    "# ================================================================================== #\n",
    "# float64 --> float32\n",
    "ds_static_1deg = ds_static_1deg.astype({var: np.float32 for var in ds_static_1deg})\n",
    "\n",
    "# Convert latitude, longitude, and level coordinates to float32\n",
    "ds_static_1deg = ds_static_1deg.assign_coords({\n",
    "    'latitude': ds_example['latitude'],\n",
    "    'longitude': ds_example['longitude'],\n",
    "    'level': ds_example['level']\n",
    "})\n",
    "\n",
    "# ================================================================================== #\n",
    "# chunk\n",
    "varnames = list(ds_static_1deg.keys())\n",
    "varnames = varnames[:-1] # subtract 'level'\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    ds_static_1deg[var] = ds_static_1deg[var].chunk({'latitude': 181, 'longitude': 360})\n",
    "\n",
    "# zarr encodings\n",
    "dict_encoding = {}\n",
    "\n",
    "chunk_size_2d = dict(chunks=(conf['zarr_opt']['chunk_size_3d_1deg']['latitude'],\n",
    "                             conf['zarr_opt']['chunk_size_3d_1deg']['longitude']))\n",
    "\n",
    "compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    dict_encoding[var] = {'compressor': compress, **chunk_size_2d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3908eba8-617e-4cd2-8a93-3cd57d2522b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x14fd5d7a1150>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_name = base_dir_1deg + 'static/ERA5_plevel_1deg_6h_conserve_static.zarr'\n",
    "# ds_static_1deg.to_zarr(save_name, mode='w', consolidated=True, compute=True, encoding=dict_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca609e1-4371-49f7-b670-a2e9c9ba8474",
   "metadata": {},
   "source": [
    "### Cloud and humidity file group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825919e3-8ff4-4687-bd33-9647800ece8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.realpath('data_config_6h.yml')\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)\n",
    "\n",
    "year = 1979 # test on one year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cbb53af-9c4e-474c-b97f-80f55d72c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = conf['zarr_opt']['save_loc']\n",
    "base_dir_1deg = conf['zarr_opt']['save_loc_1deg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786c08f1-6aa9-499c-ab33-12afb43e0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all 0.25 deg ERA5 data\n",
    "zarr_name_cloud = base_dir+'cloud/ERA5_plevel_6h_cloud_{}.zarr'\n",
    "ds_cloud = xr.open_zarr(zarr_name_cloud.format(year))\n",
    "\n",
    "# ======================================================================================= #\n",
    "# 0.25 deg to 1 deg interpolation using conservative approach\n",
    "\n",
    "# Define the target 1-degree grid\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)\n",
    "target_grid = iu.Grid.from_degrees(lon_1deg, lat_1deg)\n",
    "\n",
    "lon_025deg = ds_cloud['longitude'].values\n",
    "lat_025deg = ds_cloud['latitude'].values[::-1]\n",
    "source_grid = iu.Grid.from_degrees(lon_025deg, lat_025deg)\n",
    "\n",
    "regridder = iu.ConservativeRegridder(source=source_grid, target=target_grid)\n",
    "\n",
    "ds_cloud = ds_cloud.chunk({'longitude': -1, 'latitude': -1})\n",
    "ds_cloud_1deg = regridder.regrid_dataset(ds_cloud)\n",
    "\n",
    "# Reorder the dimensions for all variables\n",
    "for var in ds_cloud_1deg.data_vars:\n",
    "    # Get the current dimensions of the variable\n",
    "    current_dims = ds_cloud_1deg[var].dims\n",
    "    \n",
    "    # If both 'latitude' and 'longitude' are present, reorder them\n",
    "    if 'latitude' in current_dims and 'longitude' in current_dims:\n",
    "        # New order: move 'latitude' and 'longitude' to the first two positions, preserve other dimensions\n",
    "        new_order = [dim for dim in current_dims if dim not in ['latitude', 'longitude']] + ['latitude', 'longitude']\n",
    "        \n",
    "        # Transpose the variable to the new order\n",
    "        ds_cloud_1deg[var] = ds_cloud_1deg[var].transpose(*new_order)\n",
    "\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)\n",
    "\n",
    "# Add latitude and longitude as coordinates to ds_cloud_1deg\n",
    "ds_cloud_1deg = ds_cloud_1deg.assign_coords({\n",
    "    'latitude': lat_1deg,\n",
    "    'longitude': lon_1deg\n",
    "})\n",
    "\n",
    "# flip latitude from -90 --> 90 to 90 --> -90\n",
    "ds_cloud_1deg = ds_cloud_1deg.isel(latitude=slice(None, None, -1))\n",
    "\n",
    "# float64 --> float32\n",
    "ds_cloud_1deg = ds_cloud_1deg.astype(\n",
    "    {var: np.float32 for var in ds_cloud_1deg if ds_cloud_1deg[var].dtype == np.float64})\n",
    "\n",
    "# Convert latitude, longitude, and level coordinates to float32\n",
    "ds_cloud_1deg = ds_cloud_1deg.assign_coords({\n",
    "    'latitude': ds_cloud_1deg['latitude'].astype(np.float32),\n",
    "    'longitude': ds_cloud_1deg['longitude'].astype(np.float32),\n",
    "    'level': ds_cloud_1deg['level'].astype(np.float32)\n",
    "})\n",
    "\n",
    "# ========================================================================== #\n",
    "# chunking\n",
    "varnames = list(ds_cloud_1deg.keys())\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    ds_cloud_1deg[var] = ds_cloud_1deg[var].chunk(conf['zarr_opt']['chunk_size_4d_1deg'])\n",
    "\n",
    "# zarr encodings\n",
    "dict_encoding = {}\n",
    "\n",
    "chunk_size_4d = dict(chunks=(conf['zarr_opt']['chunk_size_4d_1deg']['time'],\n",
    "                             conf['zarr_opt']['chunk_size_4d_1deg']['level'],\n",
    "                             conf['zarr_opt']['chunk_size_4d_1deg']['latitude'],\n",
    "                             conf['zarr_opt']['chunk_size_4d_1deg']['longitude']))\n",
    "\n",
    "compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    dict_encoding[var] = {'compressor': compress, **chunk_size_4d}\n",
    "\n",
    "# ========================================================================== #\n",
    "# save\n",
    "save_name = base_dir_1deg + 'cloud/ERA5_plevel_1deg_6h_cloud_{}_conserve.zarr'.format(year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef92d7c-dfae-4840-b0cc-39e8716100dd",
   "metadata": {},
   "source": [
    "### Compare against bilinear interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90ab75c5-c648-479d-b45b-0d1350abbfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'land_sea_CI_mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9110a48-fa0c-4706-9f50-c817050f6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_conserve_1deg = xr.open_zarr(base_dir_1deg + 'all_in_one/ERA5_plevel_1deg_6h_1979_conserve.zarr')\n",
    "ds_bilinear_1deg = xr.open_zarr(base_dir_1deg + 'all_in_one/bilinear/ERA5_plevel_1deg_6h_1979_bilinear.zarr')\n",
    "\n",
    "blinear_np = ds_conserve_1deg[varname].isel(time=999)\n",
    "conserve_np = ds_bilinear_1deg[varname].isel(time=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9e6341e-d0d1-4264-b0f6-c24cc17a8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.pcolormesh(blinear_np, cmap=plt.cm.jet)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e64ac05b-5a9d-4d87-9921-0a1e866189a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.pcolormesh(conserve_np, cmap=plt.cm.jet)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf95a3-d5b7-420e-a548-987e6095b745",
   "metadata": {},
   "source": [
    "## Bilinear interpolation (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e108210e-f4e7-4249-ab1d-df13d851f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4bc5b5-d2fa-4683-8a72-c1bcfad67f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.realpath('data_config_6h.yml')\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)\n",
    "\n",
    "year = 1979 # test on one year\n",
    "\n",
    "base_dir = conf['zarr_opt']['save_loc']\n",
    "base_dir_1deg = conf['zarr_opt']['save_loc_1deg']\n",
    "\n",
    "# load all 0.25 deg ERA5 data\n",
    "zarr_name_surf = base_dir+'surf/ERA5_plevel_6h_surf_{}.zarr'\n",
    "zarr_name_surf_extra = base_dir+'surf/ERA5_plevel_6h_surf_extend_{}.zarr'\n",
    "zarr_name_accum = base_dir+'accum/ERA5_plevel_6h_accum_{}.zarr'\n",
    "zarr_name_forcing = base_dir+'forcing/ERA5_plevel_6h_forcing_{}.zarr'\n",
    "zarr_name_upper = base_dir+'upper_air/ERA5_plevel_6h_upper_air_{}.zarr'\n",
    "zarr_name_upper_Q = base_dir+'upper_air/ERA5_plevel_6h_Q_{}.zarr'\n",
    "\n",
    "ds_surf = xr.open_zarr(zarr_name_surf.format(year))\n",
    "ds_surf_extra = xr.open_zarr(zarr_name_surf_extra.format(year))\n",
    "ds_accum = xr.open_zarr(zarr_name_accum.format(year))\n",
    "ds_forcing = xr.open_zarr(zarr_name_forcing.format(year))\n",
    "ds_upper = xr.open_zarr(zarr_name_upper.format(year))\n",
    "ds_upper_Q = xr.open_zarr(zarr_name_upper_Q.format(year))\n",
    "\n",
    "# ========================================================================== #\n",
    "# Interpolate to 1-degree resolution using xESMF\n",
    "\n",
    "# Define the target 1-degree grid\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)[::-1]\n",
    "\n",
    "# Create target grid as an xarray Dataset\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        'latitude': (['latitude'], lat_1deg),\n",
    "        'longitude': (['longitude'], lon_1deg)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the regridder object for bilinear interpolation\n",
    "regridder = xe.Regridder(ds_merge, ds_out, 'bilinear')\n",
    "\n",
    "# Apply the regridding to interpolate all variables\n",
    "ds_merge_1deg = regridder(ds_merge)\n",
    "\n",
    "# Post-process 'land_sea_CI_mask' to ensure ocean=0, land=1, sea-ice=-1~0 after interpolation\n",
    "land_sea_CI_mask_interp = ds_merge_1deg['land_sea_CI_mask']\n",
    "\n",
    "# Apply the following logic:\n",
    "# - If value >= 0.5, set to 1 (land)\n",
    "# - If value <= -0.01, keep as is (sea-ice)\n",
    "# - Else, set to 0 (ocean)\n",
    "land_sea_CI_mask_interp = xr.where(\n",
    "    land_sea_CI_mask_interp >= 0.5,\n",
    "    1,\n",
    "    xr.where(\n",
    "        land_sea_CI_mask_interp <= -0.01,\n",
    "        land_sea_CI_mask_interp,\n",
    "        0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update the dataset\n",
    "ds_merge_1deg['land_sea_CI_mask'] = land_sea_CI_mask_interp\n",
    "\n",
    "save_name = base_dir_1deg + 'all_in_one/ERA5_plevel_1deg_6h_{}.zarr'.format(year)\n",
    "# ds_merge_1deg.to_zarr(save_name, mode='w', consolidated=True, compute=True, encoding=dict_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682689c-fe1e-4987-89c2-40845b16c2d6",
   "metadata": {},
   "source": [
    "### Prepare static file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c2bb44-f55a-4fbc-9846-0997a2ddc4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import dask\n",
    "import zarr\n",
    "import xesmf as xe\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2c2f2d6-7e14-4dab-8957-b13dff8b1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_name = os.path.realpath('data_config_6h.yml')\n",
    "\n",
    "with open(config_name, 'r') as stream:\n",
    "    conf = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b4f9f3-6102-4a8f-b5b8-92f3be473ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_1deg = conf['zarr_opt']['save_loc_1deg']\n",
    "base_dir = conf['ARCO']['save_loc'] + 'static/' \n",
    "static_name = base_dir + conf['ARCO']['prefix'] + '_static.zarr'\n",
    "\n",
    "ds_static = xr.open_zarr(static_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efbad061-7048-4335-b154-bbb06fe52b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target 1-degree grid\n",
    "lon_1deg = np.arange(0, 360, 1)\n",
    "lat_1deg = np.arange(-90, 91, 1)[::-1]\n",
    "\n",
    "# Create target grid as an xarray Dataset\n",
    "ds_out = xr.Dataset(\n",
    "    {\n",
    "        'latitude': (['latitude'], lat_1deg),\n",
    "        'longitude': (['longitude'], lon_1deg)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the regridder object for bilinear interpolation\n",
    "regridder = xe.Regridder(ds_static, ds_out, 'bilinear')\n",
    "\n",
    "# Apply the regridding to interpolate all variables\n",
    "ds_static_1deg = regridder(ds_static)\n",
    "\n",
    "# land mask interp\n",
    "land_sea_mask_interp = ds_static_1deg['land_sea_mask']\n",
    "land_sea_mask_interp = xr.where(land_sea_mask_interp >= 0.5, 1, 0)\n",
    "ds_static_1deg['land_sea_mask'] = land_sea_mask_interp\n",
    "ds_static_1deg['land_sea_mask'] = ds_static_1deg['land_sea_mask'].astype('float32')\n",
    "\n",
    "# normalize 'geopotential_at_surface'\n",
    "mean_val = float(ds_static_1deg['geopotential_at_surface'].mean(skipna=False))\n",
    "std_val = float(ds_static_1deg['geopotential_at_surface'].std(skipna=False))\n",
    "z_norm = (ds_static_1deg['geopotential_at_surface'] - mean_val)/std_val\n",
    "\n",
    "varnames = list(ds_static_1deg.keys())\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    ds_static_1deg[var] = ds_static_1deg[var].chunk({'latitude': 181, 'longitude': 360})\n",
    "\n",
    "# zarr encodings\n",
    "dict_encoding = {}\n",
    "\n",
    "chunk_size_2d = dict(chunks=(conf['zarr_opt']['chunk_size_3d_1deg']['latitude'],\n",
    "                             conf['zarr_opt']['chunk_size_3d_1deg']['longitude']))\n",
    "\n",
    "compress = zarr.Blosc(cname='zstd', clevel=1, shuffle=zarr.Blosc.SHUFFLE, blocksize=0)\n",
    "\n",
    "for i_var, var in enumerate(varnames):\n",
    "    dict_encoding[var] = {'compressor': compress, **chunk_size_2d}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "719a5c4d-8ad7-4308-896c-3c879ba49fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x15103e3ed310>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_name = base_dir_1deg + 'static/ERA5_plevel_1deg_6h_static_zarr'\n",
    "#ds_static_1deg.to_zarr(save_name, mode='w', consolidated=True, compute=True, encoding=dict_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78f803-048e-42f4-8ca8-ab36a14d1134",
   "metadata": {},
   "source": [
    "### Prepare physics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b3e669-ef74-4bf0-bd40-b2e1b575c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_phy = xr.open_zarr(\n",
    "    '/glade/derecho/scratch/ksha/CREDIT_data/ERA5_plevel_1deg/static/ERA5_plevel_1deg_6h_static.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff524dd-475d-405c-8bf4-c30d338da342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lon1d and lat1d are 1D arrays from the dataset\n",
    "lon1d = ds_phy['longitude'].values\n",
    "lat1d = ds_phy['latitude'].values\n",
    "\n",
    "# Generate 2D latitude and longitude arrays\n",
    "lon2d, lat2d = np.meshgrid(lon1d, lat1d)\n",
    "\n",
    "# Define pressure levels (p_level) as a new dimension\n",
    "p_level = np.array([   100.,    200.,    300.,    500.,    700.,   1000.,   2000.,\n",
    "                      3000.,   5000.,   7000.,  10000.,  12500.,  15000.,  17500.,\n",
    "                     20000.,  22500.,  25000.,  30000.,  35000.,  40000.,  45000.,\n",
    "                     50000.,  55000.,  60000.,  65000.,  70000.,  75000.,  77500.,\n",
    "                     80000.,  82500.,  85000.,  87500.,  90000.,  92500.,  95000.,\n",
    "                     97500., 100000.])\n",
    "\n",
    "# Add 2D longitude and latitude arrays to the dataset\n",
    "ds_phy['lon2d'] = xr.DataArray(lon2d, dims=('latitude', 'longitude'))\n",
    "ds_phy['lat2d'] = xr.DataArray(lat2d, dims=('latitude', 'longitude'))\n",
    "\n",
    "# Add pressure levels as a new variable with its own dimension\n",
    "ds_phy['p_level'] = xr.DataArray(p_level, dims=('level'))\n",
    "\n",
    "# If you want to use 'p_level' as a coordinate (e.g., to define variables that depend on it), add it as a coordinate\n",
    "ds_phy = ds_phy.assign_coords(level=('level', p_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712a9343-dddb-4b40-a16a-f804caaf2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_phy = ds_phy.drop_vars(['z_norm', 'land_sea_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e7d0128-4b16-4273-a58d-a819de846b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_phy.to_zarr('/glade/derecho/scratch/ksha/CREDIT_data/ERA5_plevel_1deg/static/ERA5_plevel_1deg_6h_physics.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27450c76-4f3d-4250-bad3-65754d89fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test = xr.open_zarr('/glade/derecho/scratch/ksha/CREDIT_data/ERA5_plevel_1deg/static/ERA5_plevel_1deg_6h_physics.zarr')\n",
    "# ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839485d-ae55-4e97-9001-5a28bab8d573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
